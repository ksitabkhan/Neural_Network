{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Deep Learning Assignment 1</center></h1>\n",
    "<h2><center> Implementation of a Deep Neural Network</center></h2>\n",
    "<h3><center> Khadija Sitabkhan - 20236001</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design of the code:\n",
    "1. We Have defined all the functions that are commonly used globally so that we dont repeat the same code again\n",
    "2. 3 Classes are defined:\n",
    "    * Logistic_Regression: This is the simple logistic regression with no hidden Layers.\n",
    "    * myNN_WithHidden: This class includes the hidden layer of the neural network. There is 1 hidden layer defined with number of nodes can be changed by changing the self.hidden parameter\n",
    "    * MyNN_RMSProp : This is the enhancement with RMS Propagation added. In RMSProp we calculate the squared gradient to adjust the weights and bias \n",
    "3. Load the Cifar Libraries : To load images of 'deer' & 'Frog' in the given data set and classify using the Neural net code designed earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Libraries\n",
    "* **Numpy** : Used for large arrays, dot product calculation, exponential calculation(sigmoid) \n",
    "* **sklearn** :  Used to call the Train test split function to split the data set randomly into training and testing data\n",
    "* **pandas** : Used to call the read_csv function in order to read data from the moons400.csv and blobs250.csv files\n",
    "* **random** : Used to call random numbers to select data using in the Stochastic gradient step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standard Scalar Function:\n",
    "\n",
    "* The aim is to distribute the data so that its standard deviation equals to **1**\n",
    "* The formula used is `x[i]=(x[i]-mean(x))/std(x)` where x[i] is the value present in the ith row, mean(x) is the mean of the entire column and std(x) is the standard deviation of the data in the entire column.\n",
    "* Alot of machine learning algorithms perform best when the data is normally distributed. Here we change the values of the data in the data set but the distribution(range) remains the same, so that the data can converge faster.[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scalar_function(X):\n",
    "    samples , attribs =  X.shape\n",
    "    for j in range(attribs):\n",
    "        for i in range(samples):\n",
    "            X[i, j] = (X[i, j] - np.mean(X[:][j])) / np.std(X[:][j])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sigmoid Function\n",
    "Sigmoid is an activation function that is used in our neural network and logistic regression implementation. \n",
    "<br> The weighted sum and bias when added together gives us a value and the actiavtion function decides whether that value will activate the neuron or not. Hence the name activation function\n",
    "<br> Sigmoid is widely used in binary classification where we can assign 0 and 1 to either of the classes[4].\n",
    "<br> There are upto 6 different activation functions:\n",
    "1. Linear function \n",
    "2. Sigmoid\n",
    "3. Tanh\n",
    "4. ReLU\n",
    "5. Leaky Relu\n",
    "6. Softmax \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_numpy(x):\n",
    "    return (1/(1+np.exp(-x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy Function\n",
    "We measure the accuracy in terms of how much percentage of out predictions was correct.\n",
    "2 Numpy arrays are passed to the function \n",
    "1. The test set output that we already have (Ground Truth)\n",
    "2. The predicted output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_test,y_predicted):\n",
    "    score=0\n",
    "    for i in range(0, len(y_predicted)-1):\n",
    "        if (y_predicted[i]==y_test[i]):\n",
    "            score=score+1\n",
    "    score = score / len(y_test)\n",
    "    print(f\"score is : {round(score*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost Function \n",
    "This cost function is used to determine the cost the algorithm has to pay to determine the predicted value. \n",
    "<br> We have calculated the cost using the formula below. Since we are using stochastic gradient descent the value of N always remains 1 Hence no summation and mean required. We take log as it is strictly monotonically increasing[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y_true,y_predicted):\n",
    "    epsilon = 1e-15\n",
    "    y_predicted_new = max(y_predicted,epsilon)\n",
    "    y_predicted_new = [min(y_predicted_new,1-epsilon)]\n",
    "    y_predicted_new = np.array(y_predicted_new)\n",
    "    return (-(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n",
    "Despite the name contains regression, Logistic Regression is a classification algorithm. We assign weights to the different classes or features in order to determine the probability of the output class. \n",
    "<br> It is binary logistic regression if we need to determine between two classes. \n",
    "<br> It is multilinear classification If we have more than 2 classes.\n",
    "\n",
    "The main formula used is \n",
    "    <center> `z(x) = b + w1.x1 + w2.x2+ ... + wn.xn` </center>\n",
    "    <br>   $$z(x) =b+ \\sum_{i=0}^N wi.xi$$ \n",
    "    <br> <center>where wi is the weight and xi is the value assigned with the i^th feature </center>\n",
    "    <br> This z(x) is the weighted sum which is then passed to an acitvation function like sigmoid which returns a probability value in the range between 0 and 1 \n",
    "    <br> The weights determined while training the data set are then used while testing the data to determine probability\n",
    "    <br> **Algorithm of Logistic regression:**\n",
    "    1. Set the learning rate and number of epochs\n",
    "    2. Initialise weight and a bias\n",
    "    3. Repeat for number of epochs:\n",
    "        * Take any random sample from the data (stochastic gradient descent)\n",
    "        * calculate the weighted sum by the formula for z(x) above\n",
    "        * claculate the sigmoid value from weighted sum.\n",
    "        * calculate error as the difference between the calculated value (output from sigmoid) and actual value.\n",
    "        * calculate delta bias as the product of initial matrix and error \n",
    "        * delta bias is the error\n",
    "        Sochastic Gradient descent Step:\n",
    "        * Adjust the weights and bias according to the delta values calculated\n",
    "     4. Calculate the weighted sum from the test data with the predicted values of weights and bias\n",
    "     5. Sigmoid on the weighted sum to return probability\n",
    "     6. If probability > 0.5 return 1 else return 0\n",
    "     7. Calculate accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1 \n",
    "#         self.w2 = 1\n",
    "        self.bias = 1\n",
    "   \n",
    "    def fit(self, X, y,learn_rate, epochs):\n",
    "        self.w1, self.bias, cost = self.stch_gradient_descent(X,y, learn_rate,epochs)\n",
    "#         print(f\"Final weights and bias: w1: {self.w1}, bias: {self.bias}, cost: {cost}\")\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        nsamples, nattribs = np.shape(X_test)\n",
    "        weighted_sum=[]\n",
    "        for i in range(0,nsamples-1):\n",
    "            x_temp=[]\n",
    "            x_temp=X_test[i].T\n",
    "            weighted_sum1 = np.dot(self.w1,x_temp) + self.bias\n",
    "            weighted_sum1 = sigmoid_numpy(weighted_sum1)\n",
    "            #print(f\"iter: {i} -- w1 : {self.w1} -- x[i]: {X_test[i]} -- weighted sum: {self.sigmoid_numpy(weighted_sum1)}\")\n",
    "            if (weighted_sum1> 0.5):\n",
    "                weighted_sum.append(1)\n",
    "            else : \n",
    "                weighted_sum.append(0)\n",
    "            \n",
    "        return weighted_sum\n",
    "    \n",
    "    def stch_gradient_descent(self, X_tem, y_true,rate, epochs):\n",
    "        (nsamples, nattribs) = np.shape(X_tem)\n",
    "        w = np.ones(shape=nattribs)\n",
    "        bias = 1\n",
    "        rate = 0.01\n",
    "\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            random_index= random.randint(0,nsamples-1) # gather a random sample from training data Stochastic Gradient\n",
    "            x_sample=X_tem[random_index]\n",
    "            y_sample=y_true[random_index]\n",
    "            y_predicted= np.dot(w,x_sample)+bias\n",
    "                    \n",
    "            y_predicted = sigmoid_numpy(y_predicted) #fun Activation function\n",
    "            loss = cost_function(y_true, y_predicted)\n",
    "            \n",
    "            w_d = x_sample.T.dot(y_predicted-y_sample)\n",
    "            bias_d = y_predicted-y_sample\n",
    "            #Update weights and bias\n",
    "            w = w - rate * w_d\n",
    "            bias = bias - rate * bias_d\n",
    "            \n",
    "            cost = np.square(y_sample - y_predicted)\n",
    "            \n",
    "         \n",
    "            \n",
    "        return w, bias, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network Implementation\n",
    "1. Step 1 : Initialisation\n",
    "    * Set Learning Rate\n",
    "    * Set epoch\n",
    "    * Initialise Weights and bias to random values\n",
    "        If **X** is number of features present in our X_train data set & **N** is number of hidden nodes,\n",
    "        from input to hidden layer : Then Weight will be an (X x N) matrix Bias_1 will be an (N x 1) matrix\n",
    "        From Hidden to output layer: Weights will be an (N x 1) matrix bias_2 will be a (1 x1) matrix\n",
    "        (This is because we have only one output node)\n",
    "2. Step 2: Repeat for maximum iterations:\n",
    "    * Since we implement Stochastic gradient descent algorithm we take one random training sample in every iteration\n",
    "    * Forward Propagation Step: \n",
    "        * Calculate the z_hidden value using the Weight1, input values and bias_1\n",
    "        * Calculate the sigmoid of z_hidden \n",
    "        * Calculate the z_output using sigmoid(z_hidden), weight2 and bias_2 \n",
    "    * Back propagation Step:\n",
    "        * Send the errors back in the reverse order (output node -> hidden -> input ) and update the weights and bias to calulate the delta values. Delta is the change that is present between the observed and expected behavior of the network\n",
    "    * Stochastic Gradient Descent Update Step\n",
    "        * Using the delta values calculated above, update the weights and bias across the network and repeat the process again. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN_WithHidden:\n",
    "    def __init__(self):\n",
    "        self.w1 = [] # Weights assigned from input layer to Hidden Layer\n",
    "        self.w2 = [] #Weights assigned from hidden layer to output layer\n",
    "        self.bias = []\n",
    "        self.bias1 = 0\n",
    "        self.hidden = 4\n",
    "        \n",
    "\n",
    "    def fit(self, X, y,learn_rate, epochs):\n",
    "\n",
    "        cost = self.stch_gradient_descent(X,y,learn_rate, epochs)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        nsamples, nattribs = np.shape(X_test)\n",
    "        weighted_sum=[]\n",
    "        final_output=[]\n",
    "        for i in range(0,nsamples-1):\n",
    "            layer1_output=[]\n",
    "            x_temp=np.dot(X_test[i],self.w1)+self.bias.T\n",
    "            \n",
    "            for j in range(self.hidden):\n",
    "                layer1_out=sigmoid_numpy(x_temp[:,j]) #Running activation function for hidden node\n",
    "                layer1_output.append(layer1_out)\n",
    "            layer1_output=np.array(layer1_output)\n",
    "            output=np.dot(layer1_output.T,self.w2)+self.bias1\n",
    "            final_output=sigmoid_numpy(output) #activation function for output node\n",
    "            if (final_output> 0.5): # Hard Threshold\n",
    "                weighted_sum.append(1)\n",
    "            else : \n",
    "                weighted_sum.append(0)\n",
    "            \n",
    "        return weighted_sum\n",
    "    \n",
    "    def delta(self,y):\n",
    "        return sigmoid_numpy(y)*(1-sigmoid_numpy(y))\n",
    "    \n",
    "\n",
    "    \n",
    "    def stch_gradient_descent(self, X_tem, y_true,rate, epochs):\n",
    "        (nsamples, nattribs) = np.shape(X_tem)\n",
    "        #Initialising weights and bias\n",
    "        self.w1 = np.random.randn(nattribs,self.hidden)\n",
    "        self.w2 = np.random.randn(self.hidden,1)\n",
    "        self.bias = np.random.randn(self.hidden,1)\n",
    "        self.bias1=np.random.randn()\n",
    "#         rate = 0.01\n",
    "        for i in range(epochs):\n",
    "            z_hidden = []\n",
    "            a_hidden = []\n",
    "            z_output = []\n",
    "            #a_output = []\n",
    "            delta_z1= []\n",
    "            random_index= random.randint(0,nsamples-1)\n",
    "            x_sample=X_tem[random_index]\n",
    "            y_sample=y_true[random_index]\n",
    "            #Forward Propagation step\n",
    "            for j in range(self.hidden): #this will generate all the \n",
    "                #output from the first hidden node. \n",
    "                #applying logistic regression on the hidden nodes\n",
    "                self.z1 = np.dot(x_sample,self.w1[:,j])+self.bias[j]\n",
    "                z_hidden.append(self.z1)\n",
    "                self.a = sigmoid_numpy(self.z1)\n",
    "                a_hidden.append(self.a)\n",
    "            a_hidden=np.array(a_hidden)\n",
    "            z_output= np.dot(a_hidden.T,self.w2)+self.bias1\n",
    "            y_predicted = sigmoid_numpy(z_output)\n",
    "            delta_z_output= y_predicted - y_sample\n",
    "            delta_bias1 = delta_z_output\n",
    "            cost = cost_function(y_sample,y_predicted)\n",
    "            \n",
    "            #Backpropagation Calculations\n",
    "            delta_w2 = np.dot(a_hidden,delta_z_output)\n",
    "            #updating bias for the hidden node:\n",
    "#             delta_bias= np.array((3))\n",
    "            for j in range(self.hidden):\n",
    "                temp = np.dot(delta_z_output,self.w2[j])\n",
    "                delta_z1_temp= self.delta(z_hidden[j])*temp\n",
    "                delta_z1.append(delta_z1_temp)\n",
    "            delta_z1 = np.array(delta_z1)\n",
    "            delta_bias = np.array(delta_z1)\n",
    "#             self.bias = self.delta1z\n",
    "            x_sample1= x_sample.reshape(nattribs,1)\n",
    "            delta_w1 = np.dot(x_sample1,delta_z1.T)\n",
    "            \n",
    "            #Stochastic Gradient Descent update step\n",
    "            self.w2 -= rate*delta_w2\n",
    "            self.bias1 -= rate*delta_bias1\n",
    "            self.w1 -= rate*delta_w1\n",
    "            self.bias -= rate*delta_bias\n",
    "            \n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Enhancing the above implementation by adding RMS(Root Mean Squared) Propagation \n",
    "We have used the same algorithm that we created above. we keep a squared gradient of each parameter as its moving average, in the stochastic gradient descent update step, we divide the current gradient by the square root of the average squared gradient. (Epsilon is added to avoid divide by zero erros)\n",
    "\n",
    "<br> \n",
    "<br>**Working of RMSProp:**\n",
    "\n",
    "* Set the value of beta (moving average parameter).\n",
    "* Calculate the squared gradient using formula: \n",
    "<br> <center> Sq_grad_Weight = (1- $\\beta$)(delta_weight)^2</center>\n",
    "<br> <center> sq_grad_bias = (1- $\\beta$)(delta_bias)^2 </center>\n",
    "* Perform the stochastic gradient descent update step for weights and bias of hidden and output layers:\n",
    "<br><center> Weights = weights  -  delta_weights/(square_root(Sq_grad_Weight) + Epsilon)</center>\n",
    "<br><center> Bias = Bias  -  delta_Bias/(square_root(Sq_grad_Bias) + Epsilon)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN_RMSProp:\n",
    "    def __init__(self):\n",
    "        self.w1 = [] # Weights assigned from input layer to Hidden Layer\n",
    "        self.w2 = [] #Weights assigned from hidden layer to output layer\n",
    "        self.bias = [] # Bias for the hidden layer\n",
    "        self.bias1 = 0 # bias for the final output\n",
    "        self.hidden = 3 # Number of hidden layers\n",
    "        self.beta = 0.9\n",
    "        self.s_del_w1=0\n",
    "        self.s_del_w2=0\n",
    "        self.s_del_b=0\n",
    "        self.s_del_b1=0\n",
    "        self.epsilon = 1e-15\n",
    "\n",
    "    def fit(self, X, y,learn_rate, epochs):\n",
    "        cost = self.stch_gradient_descent(X,y,learn_rate, epochs)\n",
    "\n",
    "#         self.w1, self.w2, self.bias,self.bias1, cost = self.stch_gradient_descent(X,y,learn_rate, epochs)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        nsamples, nattribs = np.shape(X_test)\n",
    "        weighted_sum=[]\n",
    "        final_output=[]\n",
    "        for i in range(0,nsamples-1):\n",
    "            layer1_output=[]\n",
    "            x_temp=np.dot(X_test[i],self.w1)+self.bias.T\n",
    "            \n",
    "            for j in range(self.hidden):\n",
    "                layer1_out=sigmoid_numpy(x_temp[:,j]) #Running activation function for hidden node\n",
    "                layer1_output.append(layer1_out)\n",
    "            layer1_output=np.array(layer1_output)\n",
    "            output=np.dot(layer1_output.T,self.w2)+self.bias1\n",
    "            final_output=sigmoid_numpy(output) #activation function for output node\n",
    "            if (final_output> 0.5): # Hard Threshold\n",
    "                weighted_sum.append(1)\n",
    "            else : \n",
    "                weighted_sum.append(0)\n",
    "            \n",
    "        return weighted_sum\n",
    "    \n",
    "    def delta(self,y):\n",
    "        return sigmoid_numpy(y)*(1-sigmoid_numpy(y))\n",
    "    \n",
    "    def rms_prop(self,delta_w1,delta_w2,delta_bias , delta_bias1,rate):\n",
    "        self.s_del_w1= (1-self.beta)*np.square(delta_w1)+ self.beta*self.s_del_w1\n",
    "        self.s_del_w2= (1-self.beta)*np.square(delta_w2)+ self.beta*self.s_del_w2\n",
    "        self.s_del_b= (1-self.beta)*np.square(delta_bias)+ self.beta*self.s_del_b\n",
    "        self.s_del_b1= (1-self.beta)*np.square(delta_bias1)+ self.beta*self.s_del_b1\n",
    "        \n",
    "    \n",
    "    def stch_gradient_descent(self, X_tem, y_true,rate, epochs):\n",
    "        (nsamples, nattribs) = np.shape(X_tem)\n",
    "        #Initialising weights and bias\n",
    "        self.w1 = np.random.randn(nattribs,self.hidden)\n",
    "        self.w2 = np.random.randn(self.hidden,1)\n",
    "        self.bias = np.random.randn(self.hidden,1)\n",
    "        self.bias1=np.random.randn()\n",
    "\n",
    "        for i in range(epochs):\n",
    "            z_hidden = []\n",
    "            a_hidden = []\n",
    "            z_output = []\n",
    "            delta_z1= []\n",
    "            random_index= random.randint(0,nsamples-1)\n",
    "            x_sample=X_tem[random_index]\n",
    "            y_sample=y_true[random_index]\n",
    "            #Forward Propagation step\n",
    "            for j in range(self.hidden): #this will generate all the \n",
    "                #output from the first hidden node. \n",
    "                #applying logistic regression on the hidden nodes\n",
    "                self.z1 = np.dot(x_sample,self.w1[:,j])+self.bias[j]\n",
    "                z_hidden.append(self.z1)\n",
    "                self.a = sigmoid_numpy(self.z1)\n",
    "                a_hidden.append(self.a)\n",
    "            a_hidden=np.array(a_hidden)\n",
    "            z_output= np.dot(a_hidden.T,self.w2)+self.bias1\n",
    "            y_predicted = sigmoid_numpy(z_output)\n",
    "            delta_z_output= y_predicted - y_sample\n",
    "            delta_bias1 = delta_z_output\n",
    "            cost = cost_function(y_sample,y_predicted)\n",
    "            \n",
    "            #Backpropagation Calculations\n",
    "            delta_w2 = np.dot(a_hidden,delta_z_output)\n",
    "            #updating bias for the hidden node:\n",
    "#             delta_bias= np.array((3))\n",
    "            for j in range(self.hidden):\n",
    "                temp = np.dot(delta_z_output,self.w2[j])\n",
    "                delta_z1_temp= self.delta(z_hidden[j])*temp\n",
    "                delta_z1.append(delta_z1_temp)\n",
    "            delta_z1 = np.array(delta_z1)\n",
    "            delta_bias = np.array(delta_z1)\n",
    "#             self.bias = self.delta1z\n",
    "            x_sample1= x_sample.reshape(nattribs,1)\n",
    "            delta_w1 = np.dot(x_sample1,delta_z1.T)\n",
    "            \n",
    "            #getting values for squared gradient for root mean square propagation\n",
    "            #these values are initialised to 0 in the constructor of the class and then adjusted according to the delta weights\n",
    "            self.rms_prop(delta_w1,delta_w2,delta_bias , delta_bias1,rate)\n",
    "\n",
    "\n",
    "            #Stochastic Gradient Descent update step\n",
    "            self.w2 -= rate*(delta_w2/(np.sqrt(self.s_del_w2)+self.epsilon))\n",
    "            self.bias1 -= rate*(delta_bias1/(np.sqrt(self.s_del_b1)+self.epsilon))\n",
    "            self.w1 -= rate*(delta_w1/(np.sqrt(self.s_del_w1)+self.epsilon))\n",
    "            self.bias -= rate*(delta_bias/(np.sqrt(self.s_del_b)+self.epsilon))\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling all functions and classes for Blobs 250 data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"blobs250.csv\")\n",
    "y = df['Class'].values\n",
    "\n",
    "learn_rate=0.01\n",
    "del df['Class']   # drop the 'Class' column from the dataframe\n",
    "X = df.values     # convert the remaining columns to a numpy array\n",
    "X = Scalar_function(X)\n",
    "X_train, X_test_1, y_train, y_test_1 = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Simple Logistic Regression algorithm for Blobs250 data which is linearly seperable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 100.0%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 94.74%\n"
     ]
    }
   ],
   "source": [
    "obj1 = Logistic_Regression()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "y_predicted = obj1.predict(X_test)\n",
    "accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Neural Net implemented above for Blobs250 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 100.0%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 94.74%\n"
     ]
    }
   ],
   "source": [
    "obj1 = myNN_WithHidden()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X_test)\n",
    "accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running ReLU implemented NN algorithm for Blobs250 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 100.0%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 94.74%\n"
     ]
    }
   ],
   "source": [
    "obj1 = MyNN_RMSProp()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X_test)\n",
    "accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations on the linearly Seperable data set\n",
    "From the above scores we observe that the accuracy has remained constant for linearly sperable dataset. There is a slight overfitting of data thats why we see accuracy of 100% on the training set and 94.74 on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calling all functions and classes for Moons 400 data which is not linearly seperable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"moons400.csv\")\n",
    "y = df['Class'].values\n",
    "\n",
    "learn_rate=0.01\n",
    "del df['Class']   # drop the 'Class' column from the dataframe\n",
    "X = df.values     # convert the remaining columns to a numpy array\n",
    "\n",
    "X = Scalar_function(X)\n",
    "X_train, X_test_1, y_train, y_test_1 = train_test_split(X, y, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Logistic Regression algorithm for moons400 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 77.78%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 80.0%\n"
     ]
    }
   ],
   "source": [
    "obj1 = Logistic_Regression()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "y_predicted = obj1.predict(X_test)\n",
    "accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running NN Implemented algorithm for moons400 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 76.7%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 78.33%\n"
     ]
    }
   ],
   "source": [
    "obj1 = myNN_WithHidden()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X_test)\n",
    "accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running ReLU implemented NN algorithm for moons400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 86.74%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 85.0%\n"
     ]
    }
   ],
   "source": [
    "obj1 = MyNN_RMSProp()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=50000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X_test)\n",
    "accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "<br> We know that the moons400 dataset is not linearly seperable.\n",
    "<br> We also notice that the accuracy score for training data set reduced from 77.78 (logistic) to 76.7 (NN) and then increased to 86.74 (RMSProp). \n",
    "<br> The accuracy of the test data was infact reduced from 80.0 (logistic) to 78.33 (NN) and then increased to 85.0 (RMSPRop). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the implementation on linear vs non-linear data\n",
    "\n",
    "We observe that the accuracy on the non-linear data improves on the implementation of RMS Propagation algorithm. It however does not degrade on the linearly seperable data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the CIFAR Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "#   data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. \n",
    "#           The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "#           The image is stored in row-major order, so that the first 32 entries of the array are the red channel values \n",
    "#           of the first row of the image.\n",
    "#   labels -- a list of 10000 numbers in the range 0-9. \n",
    "#             The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadCifar(filename):\n",
    "    batch1 = loadbatch(filename)\n",
    "#     print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "    # Display all keys, so we can see the ones we want\n",
    "#     print('All keys in the batch:', batch1.keys())\n",
    "    data = batch1[b'data']\n",
    "    labels = batch1[b'labels']\n",
    "#     print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "#     print (type(data))\n",
    "#     print(data.shape)\n",
    "    new_img=[]\n",
    "    new_label_list=[]\n",
    "    names = loadlabelnames()\n",
    "    for i in range(len(labels)):\n",
    "        if(b\"frog\"== names[labels[i]]):\n",
    "            new_img.append(data[i][:1024])\n",
    "            new_label_list.append(0)\n",
    "        elif(b\"deer\" == names[labels[i]]):\n",
    "            new_img.append(data[i][:1024])\n",
    "            new_label_list.append(1)\n",
    "    X_test1=np.array(new_img)\n",
    "    y_test1=np.array(new_label_list)\n",
    "#     print(np.array(new_img).shape) \n",
    "#     print(np.array(new_label_list).reshape(len(new_label_list),1).shape)\n",
    "    return X_test1 , y_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X1,y1 = LoadCifar(\"data_batch_3\")\n",
    "scaler = StandardScaler()\n",
    "# X1 = Scalar_function(X1)\n",
    "X1 = scaler.fit_transform(X1)\n",
    "X_train, X_test_1, y_train, y_test_1 = train_test_split(X1, y1, test_size=0.3)\n",
    "\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n",
    "learn_rate =0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-261c23558eb7>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/(1+np.exp(-x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 72.17%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 56.01%\n"
     ]
    }
   ],
   "source": [
    "obj = Logistic_Regression()\n",
    "obj.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj.predict(X_train)\n",
    "y_predicted = obj.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj.predict(X_test)\n",
    "y_predicted = obj.predict(X_test_1)\n",
    "accuracy(y_test_1,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 62.14%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 54.99%\n"
     ]
    }
   ],
   "source": [
    "obj1 = myNN_WithHidden()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X_test_1)\n",
    "accuracy(y_test_1,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 67.37%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 59.56%\n"
     ]
    }
   ],
   "source": [
    "obj2 = MyNN_RMSProp()\n",
    "obj2.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj2.predict(X_train)\n",
    "y_predicted = obj2.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj2.predict(X_test)\n",
    "y_predicted = obj2.predict(X_test_1)\n",
    "accuracy(y_test_1,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1,y_test1 = LoadCifar(\"test_batch\")\n",
    "X1 = scaler.fit_transform(X_test1)\n",
    "learn_rate =0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-261c23558eb7>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return (1/(1+np.exp(-x)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 72.89%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 58.15%\n"
     ]
    }
   ],
   "source": [
    "obj = Logistic_Regression()\n",
    "obj.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj.predict(X_train)\n",
    "y_predicted = obj.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj.predict(X_test)\n",
    "y_predicted = obj.predict(X1)\n",
    "accuracy(y_test1,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 57.05%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 53.0%\n"
     ]
    }
   ],
   "source": [
    "obj1 = myNN_WithHidden()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj1.predict(X_train)\n",
    "y_predicted = obj1.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X1)\n",
    "accuracy(y_test1,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Model on training Data set\n",
      "score is : 65.12%\n",
      "\n",
      "Running Model on Test data set\n",
      "score is : 60.2%\n"
     ]
    }
   ],
   "source": [
    "obj2 = MyNN_RMSProp()\n",
    "obj2.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "print(\"Running Model on training Data set\")\n",
    "obj2.predict(X_train)\n",
    "y_predicted = obj2.predict(X_train)\n",
    "accuracy(y_predicted,np.array(y_train))\n",
    "print(\"\\nRunning Model on Test data set\")\n",
    "# obj2.predict(X_test)\n",
    "y_predicted = obj2.predict(X1)\n",
    "accuracy(y_test1,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "We see that with CIFAR data set we are getting maximum score using Logistic regression for the training data set. However we see that overfitting has reduced that the difference between the accuracy score of the training dat (data_batch_3) and testng (test_batch) has reduced drastically. Best Performance is seen with the RMS Propagation implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "<br> 1.Idea and implementation of standard Scalar :  <a href=\"https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02#:~:text=turn%20to%20StandardScaler.-,StandardScaler,values%20by%20the%20standard%20deviation.&text=StandardScaler%20results%20in%20a%20distribution%20with%20a%20standard%20deviation%20equal%20to%201\" target=\"_blank\">StandardScalar</a>.\n",
    "<br>2.  Week 2 Videos and notes :  <a href=\"https://learn-eu-central-1-prod-fleet01-xythos.content.blackboardcdn.com/5cf4ce7a4424f/6653344?X-Blackboard-Expiration=1615852800000&X-Blackboard-Signature=LBv4yTw7XFYEKaYFovrhUNXvGIuNB26v8xWIL8aifck%3D&X-Blackboard-Client-Id=130002&response-cache-control=private%2C%20max-age%3D21600&response-content-disposition=inline%3B%20filename%2A%3DUTF-8%27%27DeepLearning-02-FundamentalsOfNNs-Pt1.pdf&response-content-type=application%2Fpdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210315T180000Z&X-Amz-SignedHeaders=host&X-Amz-Expires=21600&X-Amz-Credential=AKIAZH6WM4PL5M5HI5WH%2F20210315%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Signature=9528656addd3c01f68cb371a319697b5617f942a2ff72c112522e431c3ba20be\" target=\"_blank\">Week 2 Pdf</a>.\n",
    "<br>3.  Week 3 videos and notes for Neural net & ReLU implementation :  <a href=\"https://learn-eu-central-1-prod-fleet01-xythos.content.blackboardcdn.com/5cf4ce7a4424f/6739172?X-Blackboard-Expiration=1615852800000&X-Blackboard-Signature=l%2B22bmKmymDYdmpUXBm9Oazj%2Bym2MK3ZXr60kMaXO28%3D&X-Blackboard-Client-Id=130002&response-cache-control=private%2C%20max-age%3D21600&response-content-disposition=inline%3B%20filename%2A%3DUTF-8%27%27DeepLearning-03-FundamentalsOfNNs-Pt2.pdf&response-content-type=application%2Fpdf&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20210315T180000Z&X-Amz-SignedHeaders=host&X-Amz-Expires=21600&X-Amz-Credential=AKIAZH6WM4PL5M5HI5WH%2F20210315%2Feu-central-1%2Fs3%2Faws4_request&X-Amz-Signature=e8f3368bd6514b2545ef895135eb4c28ee38b843e2a60cfb86229dcfdf1210c1\" target=\"_blank\">Week 3 Pdf</a>.\n",
    "<br>4. sigmoid and other activation function definitions: <a href=\"https://www.geeksforgeeks.org/activation-functions-neural-networks/\" target=\"_blank\">Activation Functions</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
