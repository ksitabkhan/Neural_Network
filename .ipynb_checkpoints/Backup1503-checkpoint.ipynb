{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN_simple:\n",
    "    def __init__(self):\n",
    "        self.w1 = 1 \n",
    "        self.w2 = 1\n",
    "        self.bias = 0\n",
    "    def sigmoid_numpy(self,x):\n",
    "        import math\n",
    "        return (1/(1+np.exp(-x)))\n",
    "    \n",
    "    def accuracy(self,y_test,y_predicted):\n",
    "        score=0\n",
    "        for i in range(0, len(y_predicted)-1):\n",
    "            if (y_predicted[i]==y_test[i]):\n",
    "                score=score+1\n",
    "        score = score / len(y_test)\n",
    "        print(f\"score is : {round(score*100,2)}%\")\n",
    "        #return score\n",
    "    \n",
    "    def fit(self, X, y,learn_rate, epochs):\n",
    "        self.w1, self.bias, cost = self.stch_gradient_descent(X,y, learn_rate,epochs)\n",
    "        print(f\"Final weights and bias: w1: {self.w1}, bias: {self.bias}, cost: {cost}\")\n",
    "        \n",
    "    def log_loss(self,y_true, y_predicted):\n",
    "        epsilon = 1e-15\n",
    "        y_predicted_new = max(y_predicted,epsilon)\n",
    "        y_predicted_new = [min(y_predicted_new,1-epsilon)]\n",
    "        y_predicted_new = np.array(y_predicted_new)\n",
    "        return -np.mean(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new))\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        nsamples, nattribs = np.shape(X_test)\n",
    "        weighted_sum=[]\n",
    "        for i in range(0,nsamples-1):\n",
    "            x_temp=[]\n",
    "            x_temp=X_test[i].T\n",
    "            weighted_sum1 = np.dot(self.w1,x_temp) + self.bias\n",
    "            #print(f\"iter: {i} -- w1 : {self.w1} -- x[i]: {X_test[i]} -- weighted sum: {self.sigmoid_numpy(weighted_sum1)}\")\n",
    "            if (weighted_sum1> 0.5):\n",
    "                weighted_sum.append(1)\n",
    "            else : \n",
    "                weighted_sum.append(0)\n",
    "            \n",
    "        return weighted_sum\n",
    "    \n",
    "    def stch_gradient_descent(self, X_tem, y_true,rate, epochs):\n",
    "        (nsamples, nattribs) = np.shape(X_tem)\n",
    "        w = np.ones(shape=nattribs)\n",
    "        bias = 0\n",
    "        rate = 0.01\n",
    "#         cost_list = []\n",
    "#         epoch_list=[]\n",
    "        for i in range(epochs):\n",
    "            \n",
    "            random_index= random.randint(0,nsamples-1)\n",
    "            x_sample=X_tem[random_index]\n",
    "            y_sample=y_true[random_index]\n",
    "            y_predicted= np.dot(w,x_sample)+bias\n",
    "                    \n",
    "            #y_predicted = sigmoid_numpy(weighted_sum)\n",
    "            loss = self.log_loss(y_true, y_predicted)\n",
    "            \n",
    "            w_d = -(2/nsamples)*x_sample.T.dot(y_sample-y_predicted)\n",
    "            bias_d = -( 2/nsamples)*(y_sample-y_predicted)\n",
    "            \n",
    "            w = w - rate * w_d\n",
    "            bias = bias - rate * bias_d\n",
    "            \n",
    "            cost = np.square(y_sample - y_predicted)\n",
    "            \n",
    "#             if i%100==0:\n",
    "#                 cost_list.append(cost)\n",
    "#                 epoch_list.append(i)\n",
    "            \n",
    "            \n",
    "        return w, bias, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNN_WithHidden:\n",
    "    def __init__(self):\n",
    "        self.w1 = []\n",
    "        self.w2 = []\n",
    "        self.bias = []\n",
    "        self.hidden = 4\n",
    "        \n",
    "    def sigmoid_numpy(self,x):\n",
    "        return (1/(1+np.exp(-x)))\n",
    "    \n",
    "    def accuracy(self,y_test,y_predicted):\n",
    "        score=0\n",
    "        for i in range(0, len(y_predicted)-1):\n",
    "            if (y_predicted[i]==y_test[i]):\n",
    "                score=score+1\n",
    "        score = score / len(y_test)\n",
    "        print(f\"score is : {round(score*100,2)}%\")\n",
    "        #return score\n",
    "    \n",
    "    def fit(self, X, y,learn_rate, epochs):\n",
    "#         self.stch_gradient_descent(X,y,learn_rate, epochs)\n",
    "        self.w1, self.w2, self.bias,self.bias1, cost = self.stch_gradient_descent(X,y,learn_rate, epochs)\n",
    "\n",
    "        print(f\"Final weights and bias: \\nw1: {self.w1.shape}, \\nw2: {self.w2.shape} \\nbias: {self.bias.shape},\\nbias1: {self.bias1.shape} \\ncost: {cost}\")\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        nsamples, nattribs = np.shape(X_test)\n",
    "        weighted_sum=[]\n",
    "        final_output=[]\n",
    "        for i in range(0,nsamples-1):\n",
    "            layer1_output=[]\n",
    "            x_temp=np.dot(X_test[i],self.w1)+self.bias.T\n",
    "            \n",
    "            for j in range(self.hidden):\n",
    "                layer1_out=self.sigmoid_numpy(x_temp[:,j])\n",
    "                layer1_output.append(layer1_out)\n",
    "            layer1_output=np.array(layer1_output)\n",
    "            output=np.dot(layer1_output.T,self.w2)+self.bias1\n",
    "            final_output=self.sigmoid_numpy(output)\n",
    "            if (final_output> 0.5):\n",
    "                weighted_sum.append(1)\n",
    "            else : \n",
    "                weighted_sum.append(0)\n",
    "            \n",
    "        return weighted_sum\n",
    "    \n",
    "    def delta(self,y):\n",
    "        return self.sigmoid_numpy(y)*(1-self.sigmoid_numpy(y))\n",
    "    \n",
    "    def cost_function(self,y_true,y_predicted):\n",
    "        epsilon = 1e-15\n",
    "        y_predicted_new = max(y_predicted,epsilon)\n",
    "        y_predicted_new = [min(y_predicted_new,1-epsilon)]\n",
    "        y_predicted_new = np.array(y_predicted_new)\n",
    "        return (-(y_true*np.log(y_predicted_new)+(1-y_true)*np.log(1-y_predicted_new)))\n",
    "    \n",
    "    def stch_gradient_descent(self, X_tem, y_true,rate, epochs):\n",
    "        (nsamples, nattribs) = np.shape(X_tem)\n",
    "        #self.hidden= 4\n",
    "        \n",
    "        self.w1 = np.random.randn(nattribs,self.hidden)\n",
    "        self.w2 = np.random.randn(self.hidden,1)\n",
    "        self.bias = np.random.randn(self.hidden,1)\n",
    "        self.bias1=np.random.randn()\n",
    "#         rate = 0.01\n",
    "        for i in range(epochs):\n",
    "            self.z_hidden = []\n",
    "            self.a_hidden = []\n",
    "            self.z_output = []\n",
    "            self.a_output = []\n",
    "            delta_z1= []\n",
    "            random_index= random.randint(0,nsamples-1)\n",
    "            x_sample=X_tem[random_index]\n",
    "            y_sample=y_true[random_index]\n",
    "            for j in range(self.hidden): #this will generate all the output from the first hidden node. \n",
    "                #applying logistic regression on the hidden nodes\n",
    "                self.z1 = np.dot(x_sample,self.w1[:,j])+self.bias[j]\n",
    "                self.z_hidden.append(self.z1)\n",
    "                self.a = self.sigmoid_numpy(self.z1)\n",
    "                self.a_hidden.append(self.a)\n",
    "            self.a_hidden=np.array(self.a_hidden)\n",
    "            self.z_output= np.dot(self.a_hidden.T,self.w2)+self.bias1\n",
    "            y_predicted = self.sigmoid_numpy(self.z_output)\n",
    "            delta_z_output= y_predicted - y_sample\n",
    "            delta_bias1 = delta_z_output\n",
    "            cost = self.cost_function(y_sample,y_predicted)\n",
    "\n",
    "            delta_w2 = np.dot(self.a_hidden,delta_z_output)\n",
    "            #updating bias for the hidden node:\n",
    "#             delta_bias= np.array((3))\n",
    "            for j in range(self.hidden):\n",
    "                temp = np.dot(delta_z_output,self.w2[j])\n",
    "                delta_z1_temp= self.delta(self.z_hidden[j])*temp\n",
    "                delta_z1.append(delta_z1_temp)\n",
    "            delta_z1 = np.array(delta_z1)\n",
    "            delta_bias = np.array(delta_z1)\n",
    "#             self.bias = self.delta1z\n",
    "            x_sample1= x_sample.reshape(nattribs,1)\n",
    "            delta_w1 = np.dot(x_sample1,delta_z1.T)\n",
    "            \n",
    "            self.w2 -= rate*delta_w2\n",
    "            self.bias1 -= rate*delta_bias1\n",
    "            self.w1 -= rate*delta_w1\n",
    "            self.bias -= rate*delta_bias\n",
    "            \n",
    "        return self.w1,self.w2, self.bias,self.bias1, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in the batch is 4\n",
      "All keys in the batch: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "size of data in this batch: 10000 , size of labels: 10000\n",
      "<class 'numpy.ndarray'>\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "# This function taken from the CIFAR website\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Loaded in this way, each of the batch files contains a dictionary with the following elements:\n",
    "#   data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. \n",
    "#           The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. \n",
    "#           The image is stored in row-major order, so that the first 32 entries of the array are the red channel values \n",
    "#           of the first row of the image.\n",
    "#   labels -- a list of 10000 numbers in the range 0-9. \n",
    "#             The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "def loadbatch(batchname):\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    batch = unpickle(folder+\"/\"+batchname)\n",
    "    return batch\n",
    "def loadlabelnames():\n",
    "    folder = 'cifar-10-batches-py'\n",
    "    meta = unpickle(folder+\"/\"+'batches.meta')\n",
    "    return meta[b'label_names']\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualise(data, index):\n",
    "    # MM Jan 2019: Given a CIFAR data nparray and the index of an image, display the image.\n",
    "    # Note that the images will be quite fuzzy looking, because they are low res (32x32).\n",
    "\n",
    "    picture = data[index]\n",
    "    # Initially, the data is a 1D array of 3072 pixels; reshape it to a 3D array of 3x32x32 pixels\n",
    "    # Note: after reshaping like this, you could select one colour channel or average them.\n",
    "    picture.shape = (3,32,32) \n",
    "    \n",
    "    # Plot.imshow requires the RGB to be the third dimension, not the first, so need to rearrange\n",
    "    picture = picture.transpose([1, 2, 0])\n",
    "    plt.imshow(picture)\n",
    "    plt.show()\n",
    "    \n",
    "batch1 = loadbatch('data_batch_1')\n",
    "print(\"Number of items in the batch is\", len(batch1))\n",
    "\n",
    "# Display all keys, so we can see the ones we want\n",
    "print('All keys in the batch:', batch1.keys())\n",
    "data = batch1[b'data']\n",
    "labels = batch1[b'labels']\n",
    "\n",
    "print (\"size of data in this batch:\", len(data), \", size of labels:\", len(labels))\n",
    "print (type(data))\n",
    "print(data.shape)\n",
    "new_img=[]\n",
    "new_label_list=[]\n",
    "names = loadlabelnames()\n",
    "\n",
    "# Display a few images from the batch\n",
    "# for i in range (100,120):\n",
    "#    visualise(data, i)\n",
    "#    print(\"Image\", i,\": Class is \", names[labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2029, 1024)\n",
      "(2029, 1)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(labels)):\n",
    "    if(b\"frog\"== names[labels[i]]):\n",
    "        new_img.append(data[i][:1024])\n",
    "        new_label_list.append(0)\n",
    "    elif(b\"deer\" == names[labels[i]]):\n",
    "        new_img.append(data[i][:1024])\n",
    "        new_label_list.append(1)\n",
    "X1=np.array(new_img)\n",
    "y1=np.array(new_label_list)\n",
    "print(np.array(new_img).shape) \n",
    "print(np.array(new_label_list).reshape(len(new_label_list),1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weights and bias: \n",
      "w1: (1024, 4), \n",
      "w2: (4, 1) \n",
      "bias: (4, 1),\n",
      "bias1: (1, 1) \n",
      "cost: [[[0.8133824]]]\n",
      "score is : 53.77%\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X1 = scaler.fit_transform(X1)\n",
    "\n",
    "    #obj1 = Logistic_Multilinear(n_iteration=10000).train_algo(X_train, y_train)\n",
    "X_train, X_test_1, y_train, y_test_1 = train_test_split(X1, y1, test_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n",
    "learn_rate =0.01\n",
    "obj1 = myNN_WithHidden()\n",
    "obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "obj1.predict(X_test)\n",
    "y_predicted = obj1.predict(X_test)\n",
    "obj1.accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for accuracy of multiple iterations\n",
      "Final weights and bias: w1: [ 0.4208368  -0.07253345 -0.06631725], bias: 0.35071729118697387, cost: 0.1472747516526127\n",
      "score is : 55.26%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"blobs250.csv\")\n",
    "\n",
    "    # The y values are those labelled 'Class': extract their values\n",
    "    y = df['Class'].values\n",
    "\n",
    "    learn_rate=0.01\n",
    "    # The x values are all other columns\n",
    "    del df['Class']   # drop the 'Class' column from the dataframe\n",
    "    X = df.values     # convert the remaining columns to a numpy array\n",
    "    print(\"Generating report for accuracy of multiple iterations\")\n",
    "    #X1 = Scalar_function(X)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    #obj1 = Logistic_Multilinear(n_iteration=10000).train_algo(X_train, y_train)\n",
    "    X_train, X_test_1, y_train, y_test_1 = train_test_split(X, y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n",
    "    obj1 = myNN_simple()\n",
    "    obj1.fit(X_train, y_train,learn_rate, epochs=10000)\n",
    "    y_predicted = obj1.predict(X_test)\n",
    "    obj1.accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for accuracy of multiple iterations\n",
      "Final weights and bias: \n",
      "w1: (2, 4), \n",
      "w2: (4, 1) \n",
      "bias: (4, 1),\n",
      "bias1: (1, 1) \n",
      "cost: [[[0.46686284]]]\n",
      "score is : 83.33%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"moons400.csv\")\n",
    "\n",
    "    # The y values are those labelled 'Class': extract their values\n",
    "    y = df['Class'].values\n",
    "\n",
    "    learn_rate=0.01\n",
    "    # The x values are all other columns\n",
    "    del df['Class']   # drop the 'Class' column from the dataframe\n",
    "    X = df.values     # convert the remaining columns to a numpy array\n",
    "    print(\"Generating report for accuracy of multiple iterations\")\n",
    "    #X1 = Scalar_function(X)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    #obj1 = Logistic_Multilinear(n_iteration=10000).train_algo(X_train, y_train)\n",
    "    X_train, X_test_1, y_train, y_test_1 = train_test_split(X, y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n",
    "    obj1 = myNN_WithHidden()\n",
    "    obj1.fit(X_train, y_train,learn_rate, epochs=2000)\n",
    "    obj1.predict(X_test)\n",
    "    y_predicted = obj1.predict(X_test)\n",
    "    obj1.accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for accuracy of multiple iterations\n",
      "Final weights and bias: w1: [ 0.19349333 -0.27566406], bias: 0.4987372660999654, cost: 0.5718020090958449\n",
      "score is : 80.0%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"moons400.csv\")\n",
    "\n",
    "    # The y values are those labelled 'Class': extract their values\n",
    "    y = df['Class'].values\n",
    "    learn_rate=0.001\n",
    "    # The x values are all other columns\n",
    "    del df['Class']   # drop the 'Class' column from the dataframe\n",
    "    X = df.values     # convert the remaining columns to a numpy array\n",
    "    print(\"Generating report for accuracy of multiple iterations\")\n",
    "    #X1 = Scalar_function(X)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    #obj1 = Logistic_Multilinear(n_iteration=10000).train_algo(X_train, y_train)\n",
    "    X_train, X_test_1, y_train, y_test_1 = train_test_split(X, y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n",
    "    obj1 = myNN_simple()\n",
    "    obj1.fit(X_train, y_train,learn_rate, epochs=100000)\n",
    "    y_predicted = obj1.predict(X_test)\n",
    "    obj1.accuracy(y_test,np.array(y_predicted))\n",
    "#print(f\"score is : {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report for accuracy of multiple iterations\n",
      "Final weights and bias: \n",
      "w1: (3, 4), \n",
      "w2: (4, 1) \n",
      "bias: (4, 1),\n",
      "bias1: (1, 1) \n",
      "cost: [[[0.10849208]]]\n",
      "score is : 94.74%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv(\"blobs250.csv\")\n",
    "\n",
    "    # The y values are those labelled 'Class': extract their values\n",
    "    y = df['Class'].values\n",
    "\n",
    "    learn_rate=0.01\n",
    "    # The x values are all other columns\n",
    "    del df['Class']   # drop the 'Class' column from the dataframe\n",
    "    X = df.values     # convert the remaining columns to a numpy array\n",
    "    print(\"Generating report for accuracy of multiple iterations\")\n",
    "    #X1 = Scalar_function(X)\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    #obj1 = Logistic_Multilinear(n_iteration=10000).train_algo(X_train, y_train)\n",
    "    X_train, X_test_1, y_train, y_test_1 = train_test_split(X, y, test_size=0.3)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test_1, y_test_1, test_size=0.5)\n",
    "    obj1 = myNN_WithHidden()\n",
    "    obj1.fit(X_train, y_train,learn_rate, epochs=2000)\n",
    "    obj1.predict(X_test)\n",
    "    y_predicted = obj1.predict(X_test)\n",
    "    obj1.accuracy(y_test,np.array(y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
